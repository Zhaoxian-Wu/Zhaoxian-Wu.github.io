<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-76BLNR920M"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-76BLNR920M');
</script>
<meta name="google-site-verification" content="C4r0SmjTG14-FQFEYkVnw9U84lnlNDHyqvHTIWR2Bsw">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8">
<link rel="icon" href="cty.png">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Zhaoxian Wu</title>
</head>
<body>
<noscript>&lt;div class="statcounter"&gt;&lt;a title="Web Analytics"
href="https://statcounter.com/" target="_blank"&gt;&lt;img
class="statcounter"
src="https://c.statcounter.com/10867279/0/cab9fef6/1/"
alt="Web Analytics"&gt;&lt;/a&gt;&lt;/div&gt;</noscript><table summary="Table for page layout." id="tlayout">
<tbody><tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<!-- <div class="menu-item"><a href="publications.html" class="current">Publications</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>  

My research interest lies in <b>Analog In-memory Computing</b>, <b>optimization</b> and <b>machine learning</b> in <b>distributed or decentralized networks</b>. 
<h2>Training on Analog In-memory Computing Hardware</h2>

<p>Modern deep model training often requires processing vast amounts of weights and data, which must be transferred between memory and processor, leading to the <b>"von Neumann bottleneck"</b> that can significantly hinder computation speed and efficiency. In this context, <b>Analog in-memory computing (AIMC)</b> is an innovative computing paradigm that utilizes the physical properties of emerging <b>non-volatile memory (NVM)</b> devices to perform computations directly within the memory array. The core concept is to harness the analog storage and processing capabilities of NVM devices to execute <b>matrix-vector multiplication (MVM)</b> operations in a highly parallel and energy-efficient manner. </p>

<p>Training on AIMC hardware, while promising in terms of <b>energy efficiency</b> and <b>speed</b>, faces several significant challenges which comes from hardware imperfection. One major difficulty is the inherent variability and noise in analog hardware, which lead to inaccuracies in computations. Additionally, the precision of analog computations is generally lower than that of digital counterparts, making it harder to achieve the same level of accuracy required for training deep neural networks. Another challenge is the limited endurance and retention of analog memory devices, which can degrade over time and affect the reliability of the training process. Furthermore, integrating analog components with existing digital systems requires sophisticated <b>design and calibration techniques</b> to ensure compatibility and optimal performance.</p>

<p>To address these challenges, my research focuses on developing novel <b>algorithms and techniques</b> that enable effectively train deep neural networks on AIMC hardware.</p>

<img src="images/AIMC illustration.PNG" alt="alt text" width="700px" >

<h2>Distributed Optimization</h2>
Distributed or decentralized learning, which involves a series of single devices (workers) collaborating to train a machine learning model, usually serves as a promising solution in the following scenarios:
<ul>
  <li>Accelerate large-scale machine learning through parallel computation in data centers.</li>
  <li>Exploit the potential value of large-volume, heterogeneous, and privacy-sensitive data located at geographically distributed devices in settings like federated learning (FL) or multi-agent reinforcement learning (MARL).</li>
</ul>

As one of the central topics in distributed networks, my recent work focus on <b>robust distributed optimization algorithms</b>. Despite the well-known advantages, the distributed nature of networks makes them vulnerable to workersâ€™ misbehaviors, especially in FL scenarios. This misbehavior, including malicious misleading, data poisoning, backdoor injection, and so on, can be abstracted into a so-called <b>Byzantine attack</b> model, where some workers (attackers) send arbitrary malicious messages to others. Hoping to fulfill the robustness requirement, I dedicate myself to designing and analyzing Byzantine-resilient optimization algorithms. 

<br> <br><br> 

<img src="images/research-byzantine.jpg" alt="alt text" width="700px" >

<h2>Journals</h2>
<ul>

  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/11003478">Single-Timescale Multi-Sequence Stochastic Approximation Without Fixed Point Smoothness: Theories and Applications</a></p>
    <ul>
    <li><p>Yue Huang, <b>Zhaoxian Wu</b>, Shiqian Ma, Qing Ling</p></li>
    <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2025</p></li>
    </ul>
  </li>
  

  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/10354032">Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment</a></p>
    <ul>
    <li><p>Xingrong Dong, <b>Zhaoxian Wu</b>, Qing Ling, Zhi Tian</p></li>
    <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2024</p></li>
    </ul>
  </li>
    
  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/10208131">Byzantine-Resilient Decentralized Stochastic Optimization with Robust Aggregation Rules</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Tianyi Chen, Qing Ling</p></li>
  <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2023</p></li>
  </ul>
  </li>

  <li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025522012464">Byzantine-Robust Variance-Reduced Federated Learning over Distributed Non-i.i.d. Data</a></p>
    <ul>
    <li><p>Jie Peng, <b>Zhaoxian Wu</b>, Qing Ling</p></li>
    <li><p>Information Sciences, 2022</p></li>
    </ul>
  </li>
  
  <li><p><a href="https://ieeexplore.ieee.org/document/9447173">Communication-censored Distributed Stochastic Gradient Descent</a></p>
    <ul>
    <li><p>Weiyu Li, <b>Zhaoxian Wu</b>, Tianyi Chen, Liping Li, Qing Ling</p></li>
    <li><p>IEEE Transactions on Neural Networks and Learning Systems <b>(TNNLS)</b>, 2021</p></li>
    </ul>
  </li>
      
  <li><p><a href="https://arxiv.org/abs/2009.11146">Byzantine-resilient Decentralized Policy Evaluation with Linear Function Approximation</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Han Shen, Tianyi Chen, Qing Ling</p></li>
  <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2021</p></li>
  </ul>
  </li>
    
  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/9153949">Federated Variance-reduced Stochastic Gradient Descent with Robustness to Byzantine Attacks</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Qing Ling, Tianyi Chen, and Georgios B Giannakis</p></li>
  <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2020</p></li>
  </ul>
  </li>
</ul>


<h2>Conference</h2>
<ul>

  <li><p><a href="https://arxiv.org/abs/2406.12774">Towards Exact Gradient-based Training on Analog In-memory Computing</a></p>
    <ul>
    <li><p><b>Zhaoxian Wu</b>, Tayfun Gokmen, Malte J. Rasch, Tianyi Chen</p></li>
    <li><p>Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2024</p></li>
    </ul>
  </li>

  <li><p><a href="https://ieeexplore.ieee.org/document/10447185">On the Convergence of Single-Timescale Multi-Sequence Stochastic Approximation Without Fixed Point Smoothness</a></p>
    <ul>
    <li><p>Yue Huang, <b>Zhaoxian Wu</b>, Qing Ling</p></li>
    <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2024</p></li>
    </ul>
  </li>

  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/10095178">Distributed Online Learning With Adversarial Participants In An Adversarial Environment</a></p>
    <ul>
    <li><p>Xingrong Dong, <b>Zhaoxian Wu</b>, Qing Ling, Zhi Tian</p></li>
    <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2023</p></li>
    </ul>
  </li>
    
  <li><p><a href="https://ieeexplore.ieee.org/document/9747270">A Byzantine-resilient Dual Subgradient Method for Vertical Federated Learning</a></p>
    <ul>
    <li><p>Kun Yuan, <b>Zhaoxian Wu</b>, Qing Ling</p></li>
    <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2022</p></li>
    </ul>
  </li>
  
  <li><p><a href="https://ieeexplore.ieee.org/document/9413992">Byzantine-resilient Decentralized TD Learning with Linear Function Approximation</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Han Shen, Tianyi Chen, Qing Ling</p></li>
  <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2021</p></li>
  </ul>
  </li>
    
  <li><p><a href="https://ieeexplore.ieee.org/document/9053465">Byzantine-resilient Distributed Finite-sum Optimization over Networks</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Qing Ling, Tianyi Chen, and Georgios B Giannakis</p></li>
  <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2020</p></li>
  </ul>
  </li>
      

</ul>

<!-- <h2>Preprints</h2>
<ul>
  <li><p><a href="https://arxiv.org/abs/2307.07980">Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment</a></p>
    <ul>
    <li><p>Xingrong Dong, <b>Zhaoxian Wu</b>, Qing Ling, Zhi Tian</p></li>
    </ul>
  </li>

  <li><p><a href="https://arxiv.org/abs/2307.07980">Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment</a></p>
    <ul>
    <li><p>Xingrong Dong, <b>Zhaoxian Wu</b>, Qing Ling, Zhi Tian</p></li>
    </ul>
  </li>
</ul> -->

<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</tbody></table>


</body></html>
