<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-76BLNR920M"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-76BLNR920M');
</script>
<meta name="google-site-verification" content="C4r0SmjTG14-FQFEYkVnw9U84lnlNDHyqvHTIWR2Bsw">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8">
<link rel="icon" href="cty.png">
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Zhaoxian Wu</title>
</head>
<body>
<noscript>&lt;div class="statcounter"&gt;&lt;a title="Web Analytics"
href="https://statcounter.com/" target="_blank"&gt;&lt;img
class="statcounter"
src="https://c.statcounter.com/10867279/0/cab9fef6/1/"
alt="Web Analytics"&gt;&lt;/a&gt;&lt;/div&gt;</noscript><table summary="Table for page layout." id="tlayout">
<tbody><tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="research.html" class="current">Research</a></div>
<!-- <div class="menu-item"><a href="publications.html" class="current">Publications</a></div> -->
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Research</h1>
</div>  

My research interest lies in <b>Analog In-memory Computing</b>, <b>optimization</b> and <b>distributed or decentralized machine learning</b>. 
<h2>Training on Analog In-memory Computing Hardware</h2>

<p>Modern deep model training often requires processing vast amounts of weights and data, which necessitates frequent data transfer between memory and processor, leading to the <b>"von Neumann bottleneck"</b> that can significantly hinder computation speed and efficiency. In this context, <b>Analog in-memory computing (AIMC)</b> is an innovative computing paradigm that utilizes the physical properties of emerging <b>Resistive Processing Unit (RPU)</b> devices to perform computations directly within the memory array. Its core principle is to harness the analog storage and processing capabilities of these devices—leveraging the physical laws of Ohm and Kirchhoff—to execute <b>Matrix-Vector Multiplication (MVM)</b> operations in a highly parallel and energy-efficient manner.

<p>To fully realize the <b>massive parallelism and energy efficiency</b> benefits of AIMC, it is essential to perform the fully on-chip training process of Deep Neural Networks (DNNs) directly on the AIMC hardware. However, this ambitious goal faces significant challenges stemming from the inherent non-idealities of analog hardware. Key difficulties include the non-linear response of RPU devices, the pervasive presence of analog noise during computation, and the limited precision inherent in analog operations. Achieving the high accuracy required for training deep models, especially given these imperfections, remains a formidable obstacle.
</p>

<p>Crucially, since these hardware imperfections, such as device non-linearities and analog noise, are fundamentally physical and cannot be entirely eliminated in the foreseeable future, algorithmic solutions become essential. To address this reality, my research focuses on developing novel algorithms and techniques that enable the effective training of DNNs on AIMC hardware. <b>Our core objective is to establish a robust training paradigm that enables deep learning models to seamlessly "coexist" with the intrinsic imperfections of analog accelerators.</b> Specifically, we aim to develop algorithms that are inherently robust against hardware imperfections, ultimately bridging the gap between the computational demands of DNNs and the realities of non-ideal analog in-memory accelerators.
</p>

<img src="images/AIMC illustration.PNG" alt="alt text" width="700px" >

<h2>Distributed Optimization</h2>
Distributed or decentralized learning, which involves a series of single devices (workers) collaborating to train a machine learning model, usually serves as a promising solution in the following scenarios:
<ul>
  <li>Accelerate large-scale machine learning through parallel computation in data centers.</li>
  <li>Exploit the potential value of large-volume, heterogeneous, and privacy-sensitive data located at geographically distributed devices in settings like federated learning (FL) or multi-agent reinforcement learning (MARL).</li>
</ul>

As one of the central topics in distributed networks, my recent work focus on <b>robust distributed optimization algorithms</b>. Despite the well-known advantages, the distributed nature of networks makes them vulnerable to workers’ misbehaviors, especially in FL scenarios. This misbehavior, including malicious misleading, data poisoning, backdoor injection, and so on, can be abstracted into a so-called <b>Byzantine attack</b> model, where some workers (attackers) send arbitrary malicious messages to others. Hoping to fulfill the robustness requirement, I dedicate myself to designing and analyzing Byzantine-resilient optimization algorithms. 

<br> <br><br> 

<img src="images/research-byzantine.jpg" alt="alt text" width="700px" >

<h2>Journals</h2>
<ul>

  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/11050993">On the Trade-Off Between Flatness and Optimization in Distributed Learning</a></p>
    <ul>
    <li><p>Ying Cao, <b>Zhaoxian Wu</b>, Kun Yuan, Ali H. Sayed</p></li>
    <li><p>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b>, 2025</p></li>
    </ul>
  </li>

  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/11003478">Single-Timescale Multi-Sequence Stochastic Approximation Without Fixed Point Smoothness: Theories and Applications</a></p>
    <ul>
    <li><p>Yue Huang, <b>Zhaoxian Wu</b>, Shiqian Ma, Qing Ling</p></li>
    <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2025</p></li>
    </ul>
  </li>
  

  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/10354032">Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment</a></p>
    <ul>
    <li><p>Xingrong Dong, <b>Zhaoxian Wu</b>, Qing Ling, Zhi Tian</p></li>
    <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2024</p></li>
    </ul>
  </li>
    
  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/10208131">Byzantine-Resilient Decentralized Stochastic Optimization with Robust Aggregation Rules</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Tianyi Chen, Qing Ling</p></li>
  <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2023</p></li>
  </ul>
  </li>

  <li><p><a href="https://www.sciencedirect.com/science/article/abs/pii/S0020025522012464">Byzantine-Robust Variance-Reduced Federated Learning over Distributed Non-i.i.d. Data</a></p>
    <ul>
    <li><p>Jie Peng, <b>Zhaoxian Wu</b>, Qing Ling</p></li>
    <li><p>Information Sciences, 2022</p></li>
    </ul>
  </li>
  
  <li><p><a href="https://ieeexplore.ieee.org/document/9447173">Communication-censored Distributed Stochastic Gradient Descent</a></p>
    <ul>
    <li><p>Weiyu Li, <b>Zhaoxian Wu</b>, Tianyi Chen, Liping Li, Qing Ling</p></li>
    <li><p>IEEE Transactions on Neural Networks and Learning Systems <b>(TNNLS)</b>, 2021</p></li>
    </ul>
  </li>
      
  <li><p><a href="https://arxiv.org/abs/2009.11146">Byzantine-resilient Decentralized Policy Evaluation with Linear Function Approximation</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Han Shen, Tianyi Chen, Qing Ling</p></li>
  <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2021</p></li>
  </ul>
  </li>
    
  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/9153949">Federated Variance-reduced Stochastic Gradient Descent with Robustness to Byzantine Attacks</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Qing Ling, Tianyi Chen, and Georgios B Giannakis</p></li>
  <li><p>IEEE Transactions on Signal Processing <b>(TSP)</b>, 2020</p></li>
  </ul>
  </li>
</ul>


<h2>Conference</h2>
<ul>

  <li><p><a href="https://arxiv.org/abs/2502.06309">Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions</a></p>
    <ul>
    <li><p><b>Zhaoxian Wu</b>, Quan Xiao, Tayfun Gokmen, Omobayode Fagbohungbe, Tianyi Chen</p></li>
    <li><p>Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2025</p></li>
    </ul>
  </li>

  <li><p><a href="https://arxiv.org/abs/2406.12774">Towards Exact Gradient-based Training on Analog In-memory Computing</a></p>
    <ul>
    <li><p><b>Zhaoxian Wu</b>, Tayfun Gokmen, Malte J. Rasch, Tianyi Chen</p></li>
    <li><p>Conference on Neural Information Processing Systems <b>(NeurIPS)</b>, 2024</p></li>
    </ul>
  </li>

  <li><p><a href="https://ieeexplore.ieee.org/document/10447185">On the Convergence of Single-Timescale Multi-Sequence Stochastic Approximation Without Fixed Point Smoothness</a></p>
    <ul>
    <li><p>Yue Huang, <b>Zhaoxian Wu</b>, Qing Ling</p></li>
    <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2024</p></li>
    </ul>
  </li>

  <li><p><a href="https://ieeexplore.ieee.org/abstract/document/10095178">Distributed Online Learning With Adversarial Participants In An Adversarial Environment</a></p>
    <ul>
    <li><p>Xingrong Dong, <b>Zhaoxian Wu</b>, Qing Ling, Zhi Tian</p></li>
    <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2023</p></li>
    </ul>
  </li>
    
  <li><p><a href="https://ieeexplore.ieee.org/document/9747270">A Byzantine-resilient Dual Subgradient Method for Vertical Federated Learning</a></p>
    <ul>
    <li><p>Kun Yuan, <b>Zhaoxian Wu</b>, Qing Ling</p></li>
    <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2022</p></li>
    </ul>
  </li>
  
  <li><p><a href="https://ieeexplore.ieee.org/document/9413992">Byzantine-resilient Decentralized TD Learning with Linear Function Approximation</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Han Shen, Tianyi Chen, Qing Ling</p></li>
  <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2021</p></li>
  </ul>
  </li>
    
  <li><p><a href="https://ieeexplore.ieee.org/document/9053465">Byzantine-resilient Distributed Finite-sum Optimization over Networks</a></p>
  <ul>
  <li><p><b>Zhaoxian Wu</b>, Qing Ling, Tianyi Chen, and Georgios B Giannakis</p></li>
  <li><p>International Conference on Acoustics, Speech, and Signal Processing <b>(ICASSP)</b>, 2020</p></li>
  </ul>
  </li>
      

</ul>

<!-- <h2>Preprints</h2>
<ul>
  <li><p><a href="https://arxiv.org/abs/2307.07980">Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment</a></p>
    <ul>
    <li><p>Xingrong Dong, <b>Zhaoxian Wu</b>, Qing Ling, Zhi Tian</p></li>
    </ul>
  </li>

  <li><p><a href="https://arxiv.org/abs/2307.07980">Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment</a></p>
    <ul>
    <li><p>Xingrong Dong, <b>Zhaoxian Wu</b>, Qing Ling, Zhi Tian</p></li>
    </ul>
  </li>
</ul> -->

<div id="footer">
<div id="footer-text">
Page generated by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</tbody></table>


</body></html>
