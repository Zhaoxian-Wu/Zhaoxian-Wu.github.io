# Research section content
# You can edit this text to update the Research section on the home page.
html: |
  <p>My research interest lies in <b>Analog In-memory Computing</b>, and <b>optimization theory</b>. 
  </p>

  <p>Modern deep model training often requires processing vast amounts of weights and data, which necessitates frequent data transfer between memory and processor, leading to the <b>"von Neumann bottleneck"</b> that can significantly hinder computation speed and efficiency. In this context, <b>Analog in-memory computing (AIMC)</b> emerges as an innovative computing paradigm that utilizes the physical properties of emerging <b>Resistive Processing Unit (RPU)</b> devices to perform computations directly within memory arrays. Its core principle is to harness the analog storage and processing capabilities of these devices—leveraging the physical laws of Ohm and Kirchhoff—to execute <b>Matrix-Vector Multiplication (MVM)</b> operations in a highly parallel and energy-efficient manner.
  </p>

  <p>To fully realize the <b>massive parallelism and energy efficiency</b> benefits of AIMC, it is essential to perform the fully on-chip AI model training directly on the AIMC hardware. However, this ambitious goal faces significant challenges stemming from the inherent non-idealities of analog hardware. Key difficulties include the non-linear response of RPU devices, the pervasive presence of analog noise during computation, and the limited precision inherent in analog operations. Achieving the high accuracy required for training deep models, especially given these imperfections, remains a formidable obstacle.
  </p>

  <p>Crucially, since these hardware imperfections, such as device non-linearities and analog noise, are fundamentally physical and cannot be entirely eliminated in the foreseeable future, algorithmic solutions become essential. To address this reality, my research focuses on developing novel algorithms and techniques that enable the effective model training on AIMC hardware. <b>Our core objective is to establish a robust training paradigm that enables AI models to seamlessly "coexist" with the intrinsic imperfections of analog accelerators.</b> Specifically, we aim to develop algorithms that are inherently robust against hardware imperfections, ultimately bridging the gap between the computational demands of DNNs and the realities of non-ideal analog in-memory accelerators.
  </p>

  <img src="assets/research_figure/AIMC illustration.PNG" alt="alt text" width="100%" >